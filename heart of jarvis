import pyttsx3
import speech_recognition as sr
import webbrowser
import datetime
import pywhatkit
import cv2
import time
from deepface import DeepFace
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from langchain.schema import HumanMessage

# Initialize text-to-speech engine
engine = pyttsx3.init()
engine.setProperty('rate', 170)
engine.setProperty('volume', 1)

# Initialize AI chat model (Replace with your API key)
chat_model = ChatGroq(model_name="llama3-8b-8192", groq_api_key="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")

# Initialize conversation memory
memory = ConversationBufferMemory(return_messages=True)

# Load face detection model
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Global variable to store detected mood
detected_mood = "neutral"

def speak(text):
    """Convert text to speech."""
    engine.say(text)
    engine.runAndWait()

def listen():
    """Listen to user's voice and recognize speech."""
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Listening...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)
    try:
        command = recognizer.recognize_google(audio).lower()
        print(f"User: {command}")
        return command
    except sr.UnknownValueError:
        print("Sorry, I couldn't understand.")
        return ""
    except sr.RequestError:
        print("Network issue.")
        return ""

def detect_face_mood():
    """Detect mood using facial expressions for 5 seconds."""
    global detected_mood
    speak("Opening camera to analyze your mood...")
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        speak("Error: Camera not found!")
        return "neutral"

    detected_moods = []
    start_time = time.time()

    while time.time() - start_time < 5:  # Run for 5 seconds
        ret, frame = cap.read()
        if not ret:
            speak("Error: Could not capture your face!")
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        for (x, y, w, h) in faces:
            face_roi = frame[y:y + h, x:x + w]

            try:
                result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)
                emotion = result[0]['dominant_emotion']
                detected_moods.append(emotion)

                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

            except Exception as e:
                print(f"DeepFace Error: {e}")

        cv2.imshow("Mood Detection", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    # Determine most frequent detected mood
    detected_mood = max(set(detected_moods), key=detected_moods.count) if detected_moods else "neutral"

    mood_response = get_empathetic_response(detected_mood)
    speak(mood_response)
    return detected_mood

def get_empathetic_response(mood):
    """Generate an empathetic response based on the detected mood."""
    responses = {
        "happy": "You seem happy! Thatâ€™s wonderful. Keep smiling and enjoy your day!",
        "sad": "I see you're feeling sad. I'm here for you. Would you like to listen to some uplifting music?",
        "angry": "You seem upset. Take a deep breath. Maybe a relaxing activity can help.",
        "surprise": "You look surprised! Did something unexpected happen? I'm all ears!",
        "fear": "You seem scared. Don't worry, everything will be okay. Would you like some calming music?",
        "disgust": "You look uncomfortable. Maybe taking a break or watching something fun can help.",
        "neutral": "You seem neutral. Just another day, huh? Let me know if I can make it better!",
    }
    return responses.get(mood, "I'm not sure how you're feeling, but I'm here for you!")

def chat_with_memory(user_input):
    """Chat with AI while retaining past conversation context."""
    messages = memory.load_memory_variables({})['history']
    messages.append(HumanMessage(content=user_input))  # Add new user input

    response = chat_model.invoke(messages)
    memory.save_context({"input": user_input}, {"output": response.content})  # Store response in memory

    return response.content

def take_command(command):
    """Process user commands, including AI chat with memory."""
    global detected_mood

    if "hello" in command or "hey" in command:
        speak("Hello! How can I assist you today?")

    elif "detect my mood" in command:
        detected_mood = detect_face_mood()

    elif "open google" in command:
        webbrowser.open("https://google.com")
        speak("Opening Google.")

    elif "search" in command:
        search_query = command.replace("search", "").strip()
        webbrowser.open(f"https://www.google.com/search?q={search_query}")
        speak(f"Searching Google for {search_query}")

    elif "time" in command:
        time_now = datetime.datetime.now().strftime("%I:%M %p")
        speak(f"The current time is {time_now}")

    elif "date" in command:
        date_now = datetime.datetime.now().strftime("%B %d, %Y")
        speak(f"Today's date is {date_now}")

    elif "play a song" in command:
        speak("Playing a song that matches your mood.")
        pywhatkit.playonyt("happy mood songs playlist")

    elif "shutdown" in command:
        speak("Shutting down. Have a great day!")
        exit()

    elif "chat" in command:
        speak("Let's chat! What's on your mind?")
        while True:
            user_input = listen()
            if "exit chat" in user_input:
                speak("Exiting chat mode.")
                break
            response = chat_with_memory(user_input)
            speak(response)

    else:
        # If no specific command, engage in empathetic conversation
        response = chat_with_memory(command)
        speak(response)

# Start JARVIS
speak("JARVIS Activated. How can I help you?")
while True:
    command = listen()
    if command:
        take_command(command)
